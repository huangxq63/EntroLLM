{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798950dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import BertModel, BertTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc21afd",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/EntroLLM/data_wide.csv\", index_col=0)\n",
    "df.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c365ea82",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_columns = [col for col in df.columns if col.startswith('Time')]\n",
    "\n",
    "df[time_columns] = df[time_columns].astype(str)\n",
    "\n",
    "def combine(row):\n",
    "    combined_values = ' '.join(row[col] for col in time_columns)\n",
    "    return combined_values\n",
    "\n",
    "df['combined'] = df.apply(combine, axis=1)\n",
    "\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "encoding = tiktoken.get_encoding(embedding_encoding)\n",
    "df[\"n_tokens\"] = df.combined.apply(lambda x: len(encoding.encode(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5406d5ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.drop(columns=[col for col in df.columns if col.startswith('Time')])\n",
    "df.to_csv(\"E:/EntroLLM/data_wide_embedding_initial.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c156ca92",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81dca715",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"E:/EntroLLM/data_wide_embedding_initial.csv\", index_col=0)\n",
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f99e998",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### GPT ##############################\n",
    "\n",
    "# model: text-embedding-3-small\n",
    "# default embedding dimension: 1536\n",
    "\n",
    "from openai import OpenAI\n",
    "api_key = ''\n",
    "client = OpenAI(api_key=api_key)\n",
    "\n",
    "\n",
    "def get_embedding(text, model=\"text-embedding-3-small\"):\n",
    "   text = text.replace(\"\\n\", \" \")\n",
    "   return client.embeddings.create(input = [text], model=model).data[ 0].embedding\n",
    "\n",
    "embedding_model = \"text-embedding-3-small\"\n",
    "embedding_encoding = \"cl100k_base\"\n",
    "max_tokens = 8191  # the maximum for text-embedding-3-small is 8191\n",
    "\n",
    "# default dimension\n",
    "\n",
    "df_gpt=df.copy()\n",
    "df_gpt[\"embedding\"] = df_gpt.combined.apply(lambda x: get_embedding(x, model=embedding_model))\n",
    "df_gpt = df_gpt[[col for col in df.columns if not col.startswith('n_tokens') and not col.startswith('Time')]]\n",
    "df_gpt.to_csv(\"E:/EntroLLM/data_wide_embedding_gpt1536.csv\")\n",
    "df_gpt.head(2)\n",
    "\n",
    "# reduced dimension\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "def get_embeddings(texts, model=\"text-embedding-3-small\"):\n",
    "    embeddings = []\n",
    "    for text in texts:\n",
    "        response = client.embeddings.create(\n",
    "            model=model, input=text, encoding_format=\"float\"\n",
    "        )\n",
    "        embeddings.append(response.data[0].embedding[:50])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined']))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "for batch in batched_texts:\n",
    "    embeddings = get_embeddings(batch)\n",
    "    all_embeddings.extend(embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "    \n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "norm_dim = normalize_l2(all_embeddings)\n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# merge two dataset\n",
    "df_new = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_new.to_csv('E:/EntroLLM/data_wide_embedding_gpt50.csv', index=False)\n",
    "df_new.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db5f3056",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### BERT ##############################\n",
    "\n",
    "# model: bert-base-uncased\n",
    "# default embedding dimension: 768\n",
    "\n",
    "from transformers import BertTokenizer, BertModel\n",
    "import torch\n",
    "\n",
    "# Load the pre-trained BERT model and tokenizer\n",
    "model_name = 'bert-base-uncased'\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "model = BertModel.from_pretrained(model_name)\n",
    "\n",
    "\n",
    "# default dimension\n",
    "\n",
    "# Define a function to get BERT embeddings\n",
    "\n",
    "def get_bert_embeddings(text):\n",
    "    # Convert the text to the model input format\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    \n",
    "    # Disable gradient calculation\n",
    "    with torch.no_grad():\n",
    "        # Get the model outputs\n",
    "        outputs = model(**inputs)\n",
    "    \n",
    "    # Get the CLS token embeddings\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (batch_size, 768)\n",
    "    return cls_embedding.squeeze().detach().numpy()  # Shape: (768,)\n",
    "\n",
    "df_bert=df.copy()\n",
    "df_bert[\"embedding\"] = df_bert['combined'].apply(get_bert_embeddings)\n",
    "\n",
    "columns_to_drop = [col for col in df_bert.columns if col.startswith('n_tokens') or col.startswith('Time')]\n",
    "df_bert = df_bert.drop(columns=columns_to_drop)\n",
    "\n",
    "df_bert.to_csv(\"E:/EntroLLM/data_wide_embedding_bert768.csv\")\n",
    "df_bert.head(2)\n",
    "\n",
    "# reduce dimension\n",
    "\n",
    "\n",
    "def reduce_dimension(embedding, dim=50):\n",
    "    return embedding[:, :dim]\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "\n",
    "# Convert texts to embedding and reduce the dimension\n",
    "def get_bert_embeddings(text):\n",
    "    inputs = tokenizer(text, return_tensors='pt', truncation=True, padding=True)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(**inputs)\n",
    "    cls_embedding = outputs.last_hidden_state[:, 0, :]  # Shape: (1, 768)\n",
    "    reduced_embedding = reduce_dimension(cls_embedding)  # Shape: (1, 50)\n",
    "    return reduced_embedding.squeeze().detach().numpy()  # Shape: (50,)\n",
    "\n",
    "\n",
    "# Process texts in batches\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined']))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "\n",
    "for batch in batched_texts:\n",
    "    batch_embeddings = []\n",
    "    for text in batch:\n",
    "        embedding = get_bert_embeddings(text)\n",
    "        batch_embeddings.append(embedding)\n",
    "    all_embeddings.extend(batch_embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "\n",
    "\n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.array(all_embeddings)\n",
    "norm_dim = normalize_l2(all_embeddings)\n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# merge two dataset\n",
    "df_bert = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_bert.to_csv(\"E:/EntroLLM/data_wide_embedding_bert50.csv\")\n",
    "df_bert.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4da38362",
   "metadata": {},
   "outputs": [],
   "source": [
    "################### Cohere ##############################\n",
    "\n",
    "import time\n",
    "import cohere\n",
    "co = cohere.Client(\"\") \n",
    "\n",
    "\n",
    "#####################\n",
    "# split into two dataframe and run them separetly if needed\n",
    "\n",
    "# num_rows = len(df)\n",
    "# print(f\"Number of rows: {num_rows}\")\n",
    "\n",
    "# split_point = num_rows // 2\n",
    "\n",
    "# df1 = df.iloc[:split_point]\n",
    "# df2 = df.iloc[split_point:]\n",
    "#######################\n",
    "\n",
    "\n",
    "# model: embed-english-v3.0\n",
    "# default embedding dimension: 1024\n",
    "\n",
    "# default dimension\n",
    "\n",
    "def get_embeddings(texts, model='embed-english-v3.0', input_type=\"search_document\"):\n",
    "    output = co.embed(\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        texts=texts)\n",
    "    return output.embeddings\n",
    "\n",
    "# Create a new DataFrame to store embeddings\n",
    "df_cohere = df.drop(columns=['combined']).copy()\n",
    "df_cohere['embedding'] = get_embeddings(df['combined'].tolist())\n",
    "\n",
    "df_cohere.to_csv(\"E:/EntroLLM/data_wide_embedding_cohere1024.csv\")\n",
    "\n",
    "\n",
    "# reduce dimension\n",
    "\n",
    "def reduce_dimension(embedding, dim=50):\n",
    "    return embedding[:, :dim]\n",
    "\n",
    "\n",
    "# Normalize embeddings using L2 normalization\n",
    "def normalize_l2(x):\n",
    "    x = np.array(x)\n",
    "    if x.ndim == 1:\n",
    "        norm = np.linalg.norm(x)\n",
    "        if norm == 0:\n",
    "            return x\n",
    "        return x / norm\n",
    "    else:\n",
    "        norm = np.linalg.norm(x, 2, axis=1, keepdims=True)\n",
    "        return np.where(norm == 0, x, x / norm)\n",
    "\n",
    "# Get embeddings for given texts\n",
    "def get_embeddings(texts, model='embed-english-v3.0', input_type=\"search_document\",delay=0):\n",
    "    output = co.embed(\n",
    "        model=model,\n",
    "        input_type=input_type,\n",
    "        texts=texts)\n",
    "    time.sleep(delay)\n",
    "    reduced_embedding = reduce_dimension(np.array(output.embeddings))  # Shape: (batch_size, dim)\n",
    "    return reduced_embedding\n",
    "\n",
    "# Process texts in batches\n",
    "def batch_process(texts, batch_size=10):\n",
    "    n = len(texts)\n",
    "    for i in range(0, n, batch_size):\n",
    "        yield texts[i:i+batch_size]\n",
    "\n",
    "# Assuming df is your dataframe\n",
    "SEQN = df['SEQN'].tolist()\n",
    "batched_texts = list(batch_process(df['combined'].tolist()))\n",
    "all_embeddings = []\n",
    "all_SEQN = []\n",
    "\n",
    "for batch in batched_texts:\n",
    "    embeddings = get_embeddings(batch)\n",
    "    all_embeddings.append(embeddings)\n",
    "    start_index = len(all_SEQN)\n",
    "    all_SEQN.extend(SEQN[start_index:start_index + len(batch)])\n",
    "\n",
    "# Convert all embeddings to numpy array and normalize\n",
    "all_embeddings = np.vstack(all_embeddings)  # Stack embeddings vertically\n",
    "norm_dim = normalize_l2(all_embeddings)  \n",
    "\n",
    "# Create a dataframe from the normalized embeddings\n",
    "df_embeddings = pd.DataFrame(norm_dim)\n",
    "df_embeddings['SEQN'] = all_SEQN\n",
    "\n",
    "# Reorder columns to have SEQN as the first column\n",
    "cols = df_embeddings.columns.tolist()\n",
    "cols = cols[-1:] + cols[:-1]\n",
    "df_embeddings = df_embeddings[cols]\n",
    "\n",
    "df_filtered = df.loc[:, ~df.columns.str.startswith('Time')]\n",
    "\n",
    "# Merge two datasets\n",
    "df_cohere = pd.merge(df_filtered, df_embeddings, on='SEQN', how='left')\n",
    "\n",
    "df_cohere.to_csv(\"E:/EntroLLM/data_wide_embedding_cohere50.csv\")\n",
    "df_cohere.head(2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eods-f22",
   "language": "python",
   "name": "eods-f22"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
